{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BASHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BASHA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import heapq\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    formatted_text = text.lower()\n",
    "    tokens = []  # storing each words of sentence with out stop words\n",
    "    for token in nltk.word_tokenize(formatted_text):\n",
    "        tokens.append(token)\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation]\n",
    "    formatted_text = ' '.join(element for element in tokens) # creating sentence without stop words and punctations\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence is human like intelligence. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = '''artificial intelligence is human like intelligence.\n",
    "                   It is the study of intelligent artificial agents.\n",
    "                   Science and engineering to produce intelligent machines.\n",
    "                   Solve problems and have intelligence.\n",
    "                   Related to intelligent behavior.\n",
    "                   Developing of reasoning machines.\n",
    "                   Learn from mistakes and successes.\n",
    "                   Artificial intelligence is related to reasoning in everyday situations.'''\n",
    "original_text =re.sub(r'\\s+',' ',original_text) # \\s fro space and + for one or more than one space\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artificial intelligence human like intelligence',\n",
       " 'study intelligent artificial agents',\n",
       " 'science engineering produce intelligent machines',\n",
       " 'solve problems intelligence',\n",
       " 'related intelligent behavior',\n",
       " 'developing reasoning machines',\n",
       " 'learn mistakes successes',\n",
       " 'artificial intelligence related reasoning everyday situations']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_sentences  = [sentence for sentence in nltk.sent_tokenize(original_text)]\n",
    "formatted_sentnces  = [preprocess(original_sentence) for original_sentence in original_sentences]\n",
    "formatted_sentnces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_similarity(sentence1, sentence2):\n",
    "    words1 = [word for word in nltk.word_tokenize(sentence1)]\n",
    "    words2 = [word for word in nltk.word_tokenize(sentence2)]\n",
    "    \n",
    "    #print(words1)\n",
    "    #print(words2)\n",
    "    \n",
    "    all_words = list(set(words1 + words2))\n",
    "    #print(all_words)\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    for word in words1: # Bag of words\n",
    "        vector1[all_words.index(word)] += 1\n",
    "        \n",
    "    for word in words2:\n",
    "        vector2[all_words.index(word)] += 1\n",
    "   # print(vector1)\n",
    "   # print(vector2)\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(sentences):\n",
    "    similarity_matrix = np.zeros((len(sentences),len(sentences)))\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            similarity_matrix[i][j] = calculate_sentence_similarity(sentences[i], sentences[j])\n",
    "    return similarity_matrix        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.18898224, 0.        , 0.43643578, 0.        ,\n",
       "        0.        , 0.        , 0.46291005],\n",
       "       [0.18898224, 0.        , 0.2236068 , 0.        , 0.28867513,\n",
       "        0.        , 0.        , 0.20412415],\n",
       "       [0.        , 0.2236068 , 0.        , 0.        , 0.25819889,\n",
       "        0.25819889, 0.        , 0.        ],\n",
       "       [0.43643578, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23570226],\n",
       "       [0.        , 0.28867513, 0.25819889, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23570226],\n",
       "       [0.        , 0.        , 0.25819889, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.23570226],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.46291005, 0.20412415, 0.        , 0.23570226, 0.23570226,\n",
       "        0.23570226, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_similarity_matrix(formatted_sentnces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, number_of_sentences,percentage = 0):\n",
    "    original_sentences = [sentence for sentence in nltk.sent_tokenize(text)]\n",
    "    formatted_sentences = [preprocess(original_sentence) for original_sentence in original_sentences]\n",
    "    similarity_matrix = calculate_similarity_matrix(formatted_sentences)\n",
    "    #print(similarity_matrix)\n",
    "\n",
    "    similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    #print(similarity_graph.nodes)\n",
    "    #print(similarity_graph.edges)\n",
    "\n",
    "    scores = nx.pagerank(similarity_graph)\n",
    "    #print(scores)\n",
    "    ordered_scores = sorted(((scores[i], score) for i, score in enumerate(original_sentences)), reverse=True)\n",
    "    #print(ordered_scores)\n",
    "\n",
    "    if percentage > 0:\n",
    "        number_of_sentences = int(len(formatted_sentences) * percentage)\n",
    "\n",
    "    best_sentences = []\n",
    "    for sentence in range(number_of_sentences):\n",
    "        best_sentences.append(ordered_scores[sentence][1])\n",
    "    summary = \" \".join(best_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is related to reasoning in everyday situations. artificial intelligence is human like intelligence. It is the study of intelligent artificial agents.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(original_text, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
